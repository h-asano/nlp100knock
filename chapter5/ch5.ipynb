{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 係り受け解析\n",
    "---\n",
    "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 0/0 0.000000\r\n",
      "一\t名詞,数,*,*,*,*,一,イチ,イチ\r\n",
      "EOS\r\n",
      "EOS\r\n",
      "* 0 2D 0/0 -0.764522\r\n",
      "　\t記号,空白,*,*,*,*,　,　,　\r\n",
      "* 1 2D 0/1 -0.764522\r\n",
      "吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\r\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\r\n",
      "* 2 -1D 0/2 0.000000\r\n",
      "猫\t名詞,一般,*,*,*,*,猫,ネコ,ネコ\r\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\r\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\r\n",
      "。\t記号,句点,*,*,*,*,。,。,。\r\n",
      "EOS\r\n",
      "* 0 2D 0/1 -1.911675\r\n",
      "名前\t名詞,一般,*,*,*,*,名前,ナマエ,ナマエ\r\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\r\n",
      "* 1 2D 0/0 -1.911675\r\n",
      "まだ\t副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\r\n",
      "* 2 -1D 0/0 0.000000\r\n",
      "無い\t形容詞,自立,*,*,形容詞・アウオ段,基本形,無い,ナイ,ナイ\r\n",
      "。\t記号,句点,*,*,*,*,。,。,。\r\n",
      "EOS\r\n",
      "EOS\r\n"
     ]
    }
   ],
   "source": [
    "!head -25 neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラス`Morph`を実装せよ．このクラスは表層形（`surface`），基本形（`base`），品詞（`pos`），品詞細分類1（`pos1`）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文を`Morph`オブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q40.py\n"
     ]
    }
   ],
   "source": [
    "%%file q40.py\n",
    "import argparse\n",
    "from itertools import groupby\n",
    "from string import printable\n",
    "import sys\n",
    "\n",
    "\n",
    "class Morph:\n",
    "    \"\"\"cabocha lattice formatファイルの1行を読み込む\"\"\"\n",
    "    __slots__ = ('surface', 'pos', 'pos1', 'base')\n",
    "    exceptions = frozenset(printable)\n",
    "    # 吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\n",
    "    def __init__(self, line):\n",
    "        self.surface, temp = line.rstrip().split('\\t')\n",
    "        inf = temp.split(',')\n",
    "        self.pos = inf[0]\n",
    "        self.pos1 = inf[1]\n",
    "        # 4章同様, 半角文字の基本形問題\n",
    "        if self.surface in self.exceptions:\n",
    "            self.base = self.surface\n",
    "        else:\n",
    "            self.base = inf[6]\n",
    "    @classmethod\n",
    "    def load_cabocha(cls, fi):\n",
    "        \"\"\"cabocha lattice formatファイルからMorphインスタンスを生成\"\"\"\n",
    "        for is_eos, sentence in groupby(fi, key=lambda x: x == 'EOS\\n'):\n",
    "            if not is_eos:\n",
    "                yield [cls(line) for line in sentence if not line.startswith('* ')]\n",
    "                # startswith('*')だと表層形が「*」のときにまずい\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.surface\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'q40.Morph({})'.format(', '.join((self.surface, self.pos, self.pos1, self.base)))\n",
    "    \n",
    "\n",
    "def main():\n",
    "    sent_idx = arg_int()\n",
    "    for i, sent_lis in enumerate(Morph.load_cabocha(sys.stdin), start=1):\n",
    "        if i == sent_idx:\n",
    "            print(*sent_lis)\n",
    "            print(repr(sent_lis))\n",
    "            break\n",
    "\n",
    "def arg_int():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--number', default='1', type=int)\n",
    "    args = parser.parse_args()\n",
    "    return args.number\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "名前 は まだ 無い 。\r\n",
      "[q40.Morph(名前, 名詞, 一般, 名前), q40.Morph(は, 助詞, 係助詞, は), q40.Morph(まだ, 副詞, 助詞類接続, まだ), q40.Morph(無い, 形容詞, 自立, 無い), q40.Morph(。, 記号, 句点, 。)]\r\n"
     ]
    }
   ],
   "source": [
    "!python q40.py -n3 < neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q40.Morph(\", 名詞, サ変接続, \")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import q40\n",
    "q40.Morph('\"\t名詞,サ変接続,*,*,*,*,*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q40.Morph(,, 記号, 読点, ,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q40.Morph(',\t記号,読点,*,*,*,*,\",\",\",\",\",\",,')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q41.py\n"
     ]
    }
   ],
   "source": [
    "%%file q41.py\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "import sys\n",
    "\n",
    "from q40 import Morph, arg_int\n",
    "\n",
    "\n",
    "class Chunk:\n",
    "    \"\"\"cabocha lattice formatファイルから文節を読み込む\"\"\"\n",
    "    __slots__ = ('idx', 'dst', 'morphs', 'srcs')\n",
    "    \n",
    "    # * 0 2D 0/0 -0.764522\n",
    "    def __init__(self, line):\n",
    "        info = line.rstrip().split()\n",
    "        self.idx = int(info[1])\n",
    "        self.dst = int(info[2].rstrip(\"D\"))\n",
    "        self.morphs = []\n",
    "        self.srcs = []\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ''.join([morph.surface for morph in self.morphs])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'q41.Chunk({}, {})'.format(self.idx, self.dst)\n",
    "    \n",
    "    def srcs_append(self, src_idx):\n",
    "        \"\"\"係り元文節インデックスを追加\"\"\"\n",
    "        self.srcs.append(src_idx)\n",
    "    \n",
    "    def morphs_append(self, line):\n",
    "        \"\"\"形態素を追加\"\"\"\n",
    "        self.morphs.append(Morph(line))\n",
    "    \n",
    "    def chunk2str(self):\n",
    "        \"\"\"記号を取り除いた文節の表層形を返す\"\"\"\n",
    "        return ''.join([morph.surface for morph in self.morphs if morph.pos != '記号'])\n",
    "    \n",
    "    def contain_pos(self, pos):\n",
    "        \"\"\"文節中にある品詞が存在するかどうかを返す\"\"\"\n",
    "        return pos in (morph.pos for morph in self.morphs)\n",
    "\n",
    "    \n",
    "class Sentence:\n",
    "    \"\"\"cabocha lattice formatファイルから文を読み込む。Chunkクラスのヘルパー\"\"\"\n",
    "    __slots__ = ('chunks', 'idx')\n",
    "    \n",
    "    def __init__(self, sent_lines):\n",
    "        self.chunks = []\n",
    "        ch_append = self.chunks.append\n",
    "        for line in sent_lines:                    \n",
    "            if line.startswith('* '):\n",
    "                ch_append(Chunk(line))\n",
    "            else:\n",
    "                self.chunks[-1].morphs_append(line)\n",
    "        # srcsをappendしたいがためのクラス\n",
    "        for chunk in self.chunks:\n",
    "            if chunk.dst != -1:\n",
    "                self.chunks[chunk.dst].srcs_append(chunk.idx)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ' '.join([morph.surface for chunk in self.chunks for morph in chunk.morphs])\n",
    "    \n",
    "    @classmethod\n",
    "    def load_cabocha(cls, fi):\n",
    "        \"\"\"cabocha lattice formatファイルからSentenceインスタンスを生成\"\"\"\n",
    "        for is_eos, sentence in groupby(fi, key=lambda x: x == 'EOS\\n'):\n",
    "            if not is_eos:\n",
    "                yield cls(sentence)\n",
    "    \n",
    "    def print_dep_idx(self):\n",
    "        \"\"\"係り元文節インデックスと係り先文節インデックスを表示\"\"\"\n",
    "        for chunk in self.chunks:\n",
    "            print('{}:{} => {}'.format(chunk.idx, chunk, chunk.dst))\n",
    "    \n",
    "    def print_dep(self):\n",
    "        \"\"\"係り元文節と係り先文節の表層をタブ区切りで表示\"\"\"\n",
    "        for chunk in self.chunks:\n",
    "            if chunk.dst != -1:\n",
    "                print('{}\\t{}'.format(chunk.chunk2str(), self.chunks[chunk.dst].chunk2str()))\n",
    "                \n",
    "    def dep_edge(self):\n",
    "        \"\"\"pydotで係り受けを出力する用\"\"\"\n",
    "        return [(chunk.chunk2str(), self.chunks[chunk.dst].chunk2str())\n",
    "                    for chunk in self.chunks if chunk.dst != -1]\n",
    "                \n",
    "    def print_noun_verb_dep(self):\n",
    "        \"\"\"名詞を含む文節が動詞を含む文節に係るものを抽出\"\"\"\n",
    "        for chunk in self.chunks:\n",
    "            if chunk.contain_pos('名詞') and self.chunks[chunk.dst].contain_pos('動詞'):\n",
    "                print('{}\\t{}'.format(chunk.chunk2str(), self.chunks[chunk.dst].chunk2str()))\n",
    "    \n",
    "    \n",
    "    def trace_dep_path(self):\n",
    "        \"\"\"名詞を含む文節からrootまでの係り受けパスを追跡\"\"\"\n",
    "        path = []\n",
    "        ph_append = path.append\n",
    "        for chunk in self.chunks:\n",
    "            if chunk.contain_pos('名詞'):\n",
    "                ph_append(chunk)\n",
    "                d = chunk.dst\n",
    "                while d != -1:\n",
    "                    ph_append(self.chunks[d])\n",
    "                    d = self.chunks[d].dst\n",
    "                \n",
    "                yield path\n",
    "                path.clear()\n",
    "                    \n",
    "                    \n",
    "def main():\n",
    "    sent_id = arg_int()\n",
    "    for i, sent in enumerate(Sentence.load_cabocha(sys.stdin), start=1):\n",
    "        if i == sent_id:\n",
    "            sent.print_dep_idx()\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:この => 1\r\n",
      "1:書生というのは => 7\r\n",
      "2:時々 => 4\r\n",
      "3:我々を => 4\r\n",
      "4:捕えて => 5\r\n",
      "5:煮て => 6\r\n",
      "6:食うという => 7\r\n",
      "7:話である。 => -1\r\n"
     ]
    }
   ],
   "source": [
    "!python q41.py -n8 < neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q42.py\n"
     ]
    }
   ],
   "source": [
    "%%file q42.py\n",
    "import sys\n",
    "\n",
    "from q40 import arg_int\n",
    "from q41 import Sentence\n",
    "\n",
    "\n",
    "def main():\n",
    "    sent_id = arg_int()\n",
    "    for i, sent in enumerate(Sentence.load_cabocha(sys.stdin), start=1):\n",
    "        if i < sent_id:\n",
    "            sent.print_dep()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t猫である\r\n",
      "吾輩は\t猫である\r\n",
      "名前は\t無い\r\n",
      "まだ\t無い\r\n",
      "どこで\t生れたか\r\n",
      "生れたか\tつかぬ\r\n",
      "とんと\tつかぬ\r\n",
      "見当が\tつかぬ\r\n"
     ]
    }
   ],
   "source": [
    "!python q42.py -n5 < neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q43.py\n"
     ]
    }
   ],
   "source": [
    "%%file q43.py\n",
    "import sys\n",
    "\n",
    "from q40 import arg_int\n",
    "from q41 import Sentence\n",
    "\n",
    "\n",
    "def main():\n",
    "    sent_id = arg_int()\n",
    "    for i, sent in enumerate(Sentence.load_cabocha(sys.stdin), start=1):\n",
    "        if i < sent_id:\n",
    "            sent.print_noun_verb_dep()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "どこで\t生れたか\r\n",
      "見当が\tつかぬ\r\n",
      "所で\t泣いて\r\n",
      "ニャーニャー\t泣いて\r\n",
      "いた事だけは\t記憶している\r\n",
      "記憶している\t記憶している\r\n",
      "吾輩は\t見た\r\n",
      "ここで\t始めて\r\n",
      "ものを\t見た\r\n",
      "あとで\t聞くと\r\n",
      "我々を\t捕えて\r\n"
     ]
    }
   ],
   "source": [
    "!python q43.py -n10 < neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 44. 係り受け木の可視化\n",
    "\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q44.py\n"
     ]
    }
   ],
   "source": [
    "%%file q44.py\n",
    "import sys\n",
    "\n",
    "from q40 import arg_int\n",
    "from q41 import Sentence\n",
    "import pydot\n",
    "\n",
    "\n",
    "def main():\n",
    "    sent_id = arg_int()\n",
    "    for i, sent in enumerate(Sentence.load_cabocha(sys.stdin), start=1):\n",
    "        if i == sent_id:\n",
    "            edges = sent.dep_edge()\n",
    "            n = pydot.Node('node')\n",
    "            n.fontsize = 9\n",
    "            graph = pydot.graph_from_edges(edges, directed=True)\n",
    "            graph.add_node(n)\n",
    "            graph.write_jpeg(\"dep_tree_neko{}.jpg\".format(i))\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python q44.py  -n8 < neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!open dep_tree_neko8.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "dep_tree_neko8.jpg                              0%    0     0.0KB/s   --:-- ETA\r",
      "dep_tree_neko8.jpg                            100%   28KB  28.3KB/s   00:00    \r\n"
     ]
    }
   ],
   "source": [
    "!scp -P 2022 dep_tree_neko8.jpg asano@cocoa.cl.ecei.tohoku.ac.jp:/home/asano/public_html/nlp100/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neko](http://www.cl.ecei.tohoku.ac.jp/~asano/nlp100/dep_tree_neko8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```始める  で\n",
    "見る    は を```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注\n",
    "- 格を調査したいなら、格助詞を抽出するべき\n",
    "- 接続助詞「が」と格助詞「が」は別物\n",
    "- 問題文中の例でも係助詞「は」が抽出されてるのは大変まずい気が\n",
    "- 以下のコードは泣く泣く問題文に従うものとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['かかし', 'かがく']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(['かがく', 'かかし']) # 文字コードに基づく辞書式順序にしてくれる模様"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q45.py\n"
     ]
    }
   ],
   "source": [
    "%%file q45.py\n",
    "import sys\n",
    "\n",
    "from q41 import Sentence\n",
    "\n",
    "\n",
    "def main():\n",
    "    for sentence in Sentence.load_cabocha(sys.stdin):\n",
    "        case_pattern(sentence)\n",
    "        \n",
    "def case_pattern(sent):\n",
    "    for chunk in sent.chunks:\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos == '動詞':\n",
    "                verb = morph.base\n",
    "                particles = [] # 助詞のリスト\n",
    "                for src in chunk.srcs:\n",
    "                    # 分節内で一番右の助詞を追加していく\n",
    "                    particles.extend([word.base for word in sent.chunks[src].morphs \n",
    "                                         if word.pos == '助詞'][-1:])\n",
    "                particles.sort()\n",
    "                print('{}\\t{}'.format(verb, ' '.join(particles)))\n",
    "                # 一番左の動詞しか使わないのでさっさと抜ける\n",
    "                break\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    704 云う\tと\r\n",
      "    452 する\tを\r\n",
      "    435 する\t\r\n",
      "    333 思う\tと\r\n",
      "    202 ある\tが\r\n",
      "    199 なる\tに\r\n",
      "    188 する\tに\r\n",
      "    175 見る\tて\r\n",
      "    159 する\tと\r\n",
      "    122 云う\t\r\n",
      "    117 する\tが\r\n",
      "    113 する\tに を\r\n",
      "    108 なる\t\r\n",
      "     98 見る\tを\r\n",
      "     97 見える\tと\r\n",
      "     94 ある\t\r\n",
      "     90 する\tて を\r\n",
      "     89 いる\t\r\n",
      "     85 する\tは\r\n",
      "     80 見る\t\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!python q45.py < neko.txt.cabocha | sort | uniq -c | sort -rn | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    452 する\tを\n",
      "    435 する\t\n",
      "    188 する\tに\n",
      "    175 見る\tて\n",
      "    159 する\tと\n",
      "    117 する\tが\n",
      "     98 見る\tを\n",
      "     88 する\tて を\n",
      "     85 する\tは\n",
      "     80 見る\t\n",
      "     61 する\tを に\n",
      "     61 する\tて\n",
      "     60 する\tも\n",
      "     54 する\tが を\n",
      "     52 する\tに を\n",
      "     51 する\tから\n",
      "     44 する\tと を\n",
      "     44 する\tで を\n",
      "     40 する\tの\n",
      "     37 する\tは を\n",
      "sort: fflush failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "!python q45.py < neko.txt.cabocha | grep -E \"^(する|見る|与える)\\s\" | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 46. 動詞の格フレーム情報の抽出\n",
    "\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```始める  で      ここで\n",
    "見る    は を   吾輩は ものを```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('あ', 8), ('い', 5)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([('い', 5), ('あ', 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q46.py\n"
     ]
    }
   ],
   "source": [
    "%%file q46.py\n",
    "import sys\n",
    "\n",
    "from q41 import Sentence\n",
    "\n",
    "\n",
    "def main():\n",
    "    for sentence in Sentence.load_cabocha(sys.stdin):\n",
    "        pred_case_arg(sentence)\n",
    "        \n",
    "def pred_case_arg(sent):\n",
    "    for chunk in sent.chunks:\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos == '動詞':\n",
    "                verb = morph.base\n",
    "                particle_chunks = []\n",
    "                for src in chunk.srcs:\n",
    "                    # (助詞, 係り元の分節の表層)\n",
    "                    particle_chunks.extend([(word.base, sent.chunks[src].chunk2str()) \n",
    "                                            for word in sent.chunks[src].morphs if word.pos == '助詞'][-1:])\n",
    "                if particle_chunks:\n",
    "                    particle_chunks.sort()\n",
    "                    particles, chunks = zip(*particle_chunks)\n",
    "                else:\n",
    "                    particles, chunks = [], []\n",
    "                \n",
    "                print('{}\\t{}\\t{}'.format(verb, ' '.join(particles), ' '.join(chunks)))\n",
    "                break\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    435 する\t\t\r\n",
      "     28 する\tを\t顔を\r\n",
      "     22 する\tを\t事を\r\n",
      "     13 する\tに\t事に\r\n",
      "      9 する\tが\t声が\r\n",
      "      8 する\tを\t顔付を\r\n",
      "      8 する\tを\t真似を\r\n",
      "      7 する\tを\t人を\r\n",
      "      7 する\tが\t気が\r\n",
      "      6 する\tを\t運動を\r\n",
      "      6 する\tを\t懐手を\r\n",
      "      6 する\tと\tものと\r\n",
      "      5 する\tを\t知らん顔を\r\n",
      "      5 する\tに\tくらいに\r\n",
      "      5 する\tを\t話しを\r\n",
      "      5 する\tと\tあると\r\n",
      "      5 する\tに\t大に\r\n",
      "      5 する\tが\t音が\r\n",
      "      4 する\tを\t返事を\r\n",
      "      4 する\tを\t喧嘩を\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!python q46.py < neko.txt.cabocha | grep -E \"^(する)\\s\" | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 47. 機能動詞構文のマイニング\n",
    "\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```返事をする      と に は      及ばんさと 手紙に 主人は```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "- コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "- コーパス中で頻出する述語と助詞パターン\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q47.py\n"
     ]
    }
   ],
   "source": [
    "%%file q47.py\n",
    "import sys\n",
    "\n",
    "from q41 import Sentence\n",
    "\n",
    "\n",
    "def main():\n",
    "    for sentence in Sentence.load_cabocha(sys.stdin):\n",
    "        sahen_case_arg(sentence)\n",
    "        \n",
    "def sahen_case_arg(sent):\n",
    "    # サ変名詞＋動詞を抽出するためのフラグ\n",
    "    sahen_flag = 0\n",
    "    for chunk in sent.chunks:\n",
    "        for morph in chunk.morphs:\n",
    "            if sahen_flag == 0 and morph.pos1 == 'サ変接続':\n",
    "                sahen_flag = 1\n",
    "                sahen = morph.surface\n",
    "            elif sahen_flag == 1 and morph.base == 'を' and morph.pos == '助詞':\n",
    "                sahen_flag = 2\n",
    "            elif sahen_flag == 2 and morph.pos == '動詞':\n",
    "                sahen_wo = sahen + 'を'\n",
    "                verb = morph.base\n",
    "                particle_chunks = []\n",
    "                for src in chunk.srcs:\n",
    "                    # (助詞, 係り元の分節の表層)\n",
    "                    particle_chunks.extend([(word.base, sent.chunks[src].chunk2str()) for word in sent.chunks[src].morphs \n",
    "                                     if word.pos == '助詞'][-1:])\n",
    "                for j, part_chunk in enumerate(particle_chunks[:]):\n",
    "                    if sahen_wo in part_chunk[1]:\n",
    "                        del particle_chunks[j]\n",
    "                \n",
    "                if particle_chunks:\n",
    "                    particle_chunks.sort()\n",
    "                    particles, chunks = zip(*particle_chunks)\n",
    "                else:\n",
    "                    particles, chunks = [], []\n",
    "\n",
    "                print('{}\\t{}\\t{}'.format(sahen_wo + verb, ' '.join(particles), ' '.join(chunks)))\n",
    "                sahen_flag = 0\n",
    "                break\n",
    "            else:\n",
    "                sahen_flag = 0\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8 真似をする\t\t\r\n",
      "      6 喧嘩をする\t\t\r\n",
      "      4 運動をする\t\t\r\n",
      "      4 返事をする\t\t\r\n",
      "      4 話を聞く\t\t\r\n",
      "      3 話をする\t\t\r\n",
      "      2 御無沙汰をする\t\t\r\n",
      "      2 休養を要する\tは\t吾輩はまた\r\n",
      "      2 深入りをする\t\t\r\n",
      "      2 遠慮をする\t\t\r\n",
      "sort: fflush failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!python q47.py < neko.txt.cabocha | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut -f 1 work/case-pattern-wo.txt | sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を```\"->\"```で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q48.py\n"
     ]
    }
   ],
   "source": [
    "%%file q48.py\n",
    "import sys\n",
    "\n",
    "from q40 import arg_int\n",
    "from q41 import Sentence\n",
    "\n",
    "\n",
    "def main():\n",
    "    sent_id = arg_int()\n",
    "    for i, sent in enumerate(Sentence.load_cabocha(sys.stdin), start=1):\n",
    "        if i == sent_id:\n",
    "            for chunks in sent.trace_dep_path():\n",
    "                print(' -> '.join([chunk.chunk2str() for chunk in chunks]))\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は -> 見た\r\n",
      "ここで -> 始めて -> 人間という -> ものを -> 見た\r\n",
      "人間という -> ものを -> 見た\r\n",
      "ものを -> 見た\r\n"
     ]
    }
   ],
   "source": [
    "!python q48.py -n6 < neko.txt.cabocha "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 49. 名詞間の係り受けパスの抽出\n",
    "\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\n",
    "とj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
    "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節j\n",
    "のパスを表示\n",
    "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を\"|\"で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q49.py\n"
     ]
    }
   ],
   "source": [
    "%%file q49.py\n",
    "#途中\n",
    "import sys\n",
    "from itertools import combinations\n",
    "\n",
    "from q40 import arg_int\n",
    "from q41 import Sentence\n",
    "\n",
    "\n",
    "def main():\n",
    "    sent_id = arg_int()\n",
    "    for i, sent in enumerate(Sentence.load_cabocha(sys.stdin), start=1):\n",
    "        if i == sent_id:\n",
    "            for chunks in sent.trace_dep_path():\n",
    "                for c1, c2 in combinations(chunks, 2):\n",
    "                    if c2.contain_pos('名詞'):\n",
    "                        \n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人間という\r\n",
      "ものを\r\n",
      "人間という\r\n",
      "ものを\r\n",
      "ものを\r\n",
      "ものを\r\n"
     ]
    }
   ],
   "source": [
    "!python q49.py -n6 < neko.txt.cabocha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
